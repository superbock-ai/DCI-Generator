# DCI Generator - Insurance Document Analysis Tool

A comprehensive AI-powered tool for analyzing insurance documents using a three-tier hierarchical approach: **Segments** â†’ **Benefits** â†’ **Details** (limits/conditions/exclusions).

## ğŸ” Overview

The DCI (Domain Context Item) Generator performs intelligent, context-aware analysis of German insurance documents by:

1. **Identifying coverage segments** (e.g., luggage coverage, home assistance)
2. **Finding specific benefits** within discovered segments (e.g., missed connections, emergency purchases)
3. **Extracting detailed information** about limits, conditions, and exclusions for found benefits

## ğŸš€ Features

### Core Capabilities
- **Three-tier conditional analysis** - Only analyzes what exists in the document
- **Parallel processing** - Maximum speed at each analysis tier
- **Context-aware prompts** - Each tier builds on previous analysis results
- **Comprehensive caching** - Near-instant repeated analyses
- **Live GraphQL integration** - Real-time taxonomy data from Quinsights platform
- **Hierarchical tree output** - Natural taxonomy structure with nested relationships

### Advanced Features
- **Full document analysis** - Not limited to specific sections
- **German insurance expertise** - Specialized prompts for German AVB documents
- **Flexible export options** - JSON export and detailed console output
- **Environment-based configuration** - Secure credential management
- **Chunked parallel processing** - Configurable chunk sizes per analysis tier
- **Smart rate limiting** - Automatic retry with OpenAI wait time parsing
- **Debug mode with auto-resume** - Save progress and resume from failures
- **Token-aware processing** - Handles OpenAI response length limits
- **Robust error handling** - Retry logic and graceful failure recovery

## âš™ï¸ Requirements

- Python 3.12+
- OpenAI API access
- Quinsights GraphQL endpoint access
- `uv` package manager

## ğŸ› ï¸ Installation

1. **Clone and setup**:
   ```bash
   cd dci_generator
   uv sync
   ```

2. **Configure environment**:
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Required environment variables**:
   ```env
   OPENAI_API_KEY=your-openai-api-key
   GRAPHQL_AUTH_TOKEN=your-graphql-token
   OPENAI_MODEL=gpt-4o-mini  # optional
   GRAPHQL_URL=https://app-uat.quinsights.tech/graphql  # optional
   ```

## ğŸ¯ Usage

### Basic Analysis
```bash
uv run main.py travel-insurance/markdown/axa.md
```

### With Options
```bash
# Export results to JSON
uv run main.py travel-insurance/markdown/axa.md --export

# Show detailed analysis
uv run main.py travel-insurance/markdown/axa.md --detailed

# Disable caching for fresh results
uv run main.py travel-insurance/markdown/axa.md --no-cache

# All options combined
uv run main.py travel-insurance/markdown/axa.md --export --detailed --no-cache
```

### Chunk Size Configuration
```bash
# Default chunk sizes (8 segments, 8 benefits, 3 details)
uv run main.py document.md

# Conservative for reliability
uv run main.py document.md --segment-chunks 5 --benefit-chunks 4 --detail-chunks 2

# Aggressive for speed
uv run main.py document.md --segment-chunks 15 --benefit-chunks 12 --detail-chunks 5

# Token-safe for large responses
uv run main.py document.md --detail-chunks 1
```

### Debug Mode & Auto-Resume
```bash
# Enable debug mode (saves intermediate results)
uv run main.py document.md --debug

# Clean all debug files and start fresh
uv run main.py document.md --debug --debug-clean

# Force re-run from specific tier onwards
uv run main.py document.md --debug --debug-from benefits

# Ideal for token limit issues - resume with smaller chunks
uv run main.py document.md --debug --detail-chunks 1
```

### Available Documents
The system includes sample documents from major Swiss insurers:
- `allianz.md`, `axa.md`, `css.md`, `erv.md`
- `generali.md`, `mobiliar.md`, `swica.md`, `zurich.md`

## ğŸ“Š Analysis Flow

### Three-Tier Conditional Processing

```
ğŸ“„ DOCUMENT
    â†“
ğŸ” TIER 1: Segment Analysis (Parallel)
   âœ“ luggage_travel_delay â†’ FOUND
   âœ— home_assistance â†’ NOT FOUND
    â†“ (only analyze benefits for found segments)
ğŸ¯ TIER 2: Benefit Analysis (Parallel)
   âœ“ missed_connection â†’ FOUND
   âœ— essential_purchases â†’ NOT FOUND
    â†“ (only analyze details for found benefits)
ğŸ“‹ TIER 3: Detail Analysis (Parallel)
   âœ“ limits: maximum_coverage_amount â†’ FOUND
   âœ— conditions: documentation_requirements â†’ NOT FOUND
   âœ— exclusions: war_terrorism â†’ NOT FOUND
    â†“
ğŸ† HIERARCHICAL RESULTS
```

### New Hierarchical Output Structure
```json
{
  "segments": [
    {
      "luggage_travel_delay": {
        "item_name": "luggage_travel_delay",
        "is_included": true,
        "section_reference": "Modul I - ReisegepÃ¤ck",
        "description": "Coverage for luggage loss and travel delays",
        "value": "Travel luggage coverage area",
        "unit": "N/A",
        "benefits": [
          {
            "missed_connection": {
              "item_name": "missed_connection",
              "is_included": true,
              "description": "Reimbursement for missed connections",
              "value": "Additional costs for alternative transport",
              "unit": "CHF",
              "limits": [
                {
                  "maximum_coverage_amount": {
                    "item_name": "maximum_coverage_amount",
                    "is_included": true,
                    "description": "Maximum reimbursement limit",
                    "value": "5000",
                    "unit": "CHF"
                  }
                }
              ],
              "conditions": [],
              "exclusions": []
            }
          }
        ]
      }
    }
  ]
}
```

## ğŸ—ï¸ Architecture

### Core Components

- **DocumentAnalyzer**: Main analysis orchestrator
- **AnalysisResult**: Universal Pydantic model for all analysis items
- **GraphQL Integration**: Live taxonomy data retrieval
- **LangChain Chains**: Structured LLM processing with caching

### Data Sources

- **GraphQL Endpoint**: Quinsights taxonomy hierarchy
- **Document Repository**: Markdown-formatted insurance documents
- **LLM Instructions**: Specialized prompts for German insurance analysis

### Processing Pipeline

1. **Taxonomy Fetching**: Retrieve complete hierarchy from GraphQL
2. **Chain Creation**: Build analysis chains for each tier
3. **Conditional Processing**: Only process items that exist
4. **Parallel Execution**: Maximize speed with simultaneous processing
5. **Context Integration**: Each tier uses previous results as context
6. **Tree Assembly**: Build hierarchical structure with taxonomy-aligned keys

## ğŸ”§ Technical Details

### Pydantic Schema (AnalysisResult)
```python
class AnalysisResult(BaseModel):
    section_reference: str      # Document section location
    full_text_part: str        # Extracted text passage
    llm_summary: str           # AI-generated analysis
    item_name: str             # Name of analyzed item
    is_included: bool          # Coverage determination
    description: str           # LLM-extracted description
    unit: str                  # Unit of measurement
    value: str                 # Specific value found
```

### Intelligent Caching
- **Cache Scope**: All LLM calls across all tiers
- **Cache Keys**: Document content + analysis context + model parameters
- **Performance**: Near-instant repeated analyses
- **Development**: Perfect for iterative prompt development

### Error Handling
- **GraphQL Errors**: Connection and authentication issues
- **Document Errors**: File not found, encoding problems
- **LLM Errors**: Rate limits, API failures
- **Processing Errors**: Chain execution failures

## âš™ï¸ Configuration

### Environment Variables
| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | Yes | - | OpenAI API authentication |
| `GRAPHQL_AUTH_TOKEN` | Yes | - | Quinsights GraphQL token |
| `OPENAI_MODEL` | No | `gpt-4o-mini` | OpenAI model to use |
| `GRAPHQL_URL` | No | UAT endpoint | GraphQL endpoint URL |

### Command Line Options
| Option | Short | Description |
|--------|-------|-------------|
| `--export` | `-e` | Export results to JSON file |
| `--detailed` | `-d` | Show comprehensive analysis details |
| `--no-cache` | - | Disable caching for fresh results |
| `--segment-chunks` | - | Segments per parallel chunk (default: 8) |
| `--benefit-chunks` | - | Benefits per parallel chunk (default: 8) |
| `--detail-chunks` | - | Details per parallel chunk (default: 3) |
| `--debug` | - | Enable debug mode with auto-resume |
| `--debug-clean` | - | Delete all debug files before running |
| `--debug-from` | - | Force re-run from specific tier (segments/benefits/details) |

### Rate Limiting & Reliability
- **Chunked Processing**: Processes items in configurable batches to avoid API limits
- **Smart Wait Time Parsing**: Extracts exact wait times from OpenAI error messages
- **Exponential Backoff Fallback**: Falls back to exponential backoff if parsing fails
- **6 Retry Attempts**: Industry standard retry count with detailed logging
- **Chunk-Level Retries**: Only retries failed chunks, not entire batches
- **Guaranteed Completion**: Script completes successfully even with rate limit hits

### Debug Mode & Auto-Resume
- **Automatic Save**: Saves intermediate results after each successful analysis tier
- **Smart Resume**: Automatically loads existing debug files and resumes from last incomplete tier
- **Flexible Resumption**: Uses existing debug files regardless of chunk size changes between runs
- **Per-Document Files**: `document_segments.debug.json`, `document_benefits.debug.json`, `document_details.debug.json`
- **Progress Visibility**: Clear logging shows what's loaded vs. what's running
- **Failure Recovery**: Resume from failures with different configurations (e.g., smaller chunk sizes)

## ğŸ“ Project Structure

```
dci_generator/
â”œâ”€â”€ main.py                          # Main CLI application
â”œâ”€â”€ dev.ipynb                        # Development notebook
â”œâ”€â”€ graphql/
â”‚   â””â”€â”€ GetCompleteTaxonomyHierarchy.graphql  # GraphQL query
â”œâ”€â”€ travel-insurance/
â”‚   â””â”€â”€ markdown/                    # Insurance documents
â”œâ”€â”€ segment_structured_output.json   # Output schema definition
â”œâ”€â”€ example_gql_response.json       # Sample GraphQL response
â”œâ”€â”€ .env.example                    # Environment template
â”œâ”€â”€ .env                           # Environment configuration (gitignored)
â”œâ”€â”€ CLAUDE.md                      # Developer documentation
â””â”€â”€ README.md                      # This file
```

## ğŸ”’ Security

- **Environment Variables**: All secrets in `.env` file
- **Git Exclusion**: Credentials never committed to repository
- **Token Validation**: Startup verification of required credentials
- **Error Sanitization**: Sensitive information filtered from logs

## âš¡ Performance

### Optimization Features
- **Conditional Processing**: Only analyzes relevant items (typically 10-30% of total taxonomy)
- **Parallel Execution**: All items at each tier processed simultaneously
- **Intelligent Caching**: Repeated analyses served from memory
- **Efficient Data Structures**: Organized taxonomy for fast lookups

### Typical Performance
- **Initial Analysis**: 30-60 seconds (depends on document complexity)
- **Cached Analysis**: 2-5 seconds (near-instant for repeated items)
- **Memory Usage**: ~50-100MB (including cached results)

## ğŸ”¬ Development

### Development Workflow
1. **Interactive Development**: Use `dev.ipynb` for experimentation
2. **Production Testing**: Use `main.py` for complete pipeline testing
3. **Prompt Iteration**: Leverage caching for rapid prompt development
4. **Schema Evolution**: Update `AnalysisResult` model as needed

### Adding New Analysis Items
1. **GraphQL Schema**: Add new taxonomy items to GraphQL endpoint
2. **Prompt Templates**: Create specialized analysis prompts
3. **Chain Integration**: Add to parallel processing pipeline
4. **Result Processing**: Update export and display logic

## ğŸš€ Future Enhancements

### Planned Features
- **Multi-language Support**: Extend beyond German insurance documents
- **Custom Taxonomies**: Support for user-defined analysis hierarchies
- **Batch Processing**: Analyze multiple documents simultaneously
- **API Interface**: REST API for programmatic access
- **Dashboard**: Web interface for analysis management

### Extensibility Points
- **New Document Formats**: PDF, HTML, DOCX support
- **Additional LLM Providers**: Azure OpenAI, Anthropic, local models
- **Export Formats**: Excel, CSV, PDF reports
- **Integration APIs**: Webhook notifications, database storage

## ğŸ†˜ Support

For technical issues:
1. Check environment configuration (`.env` file)
2. Verify network access to GraphQL endpoint
3. Validate OpenAI API quota and permissions
4. Review error messages for specific guidance

## ğŸ“„ License

This project is proprietary software for Quinsights insurance document analysis.